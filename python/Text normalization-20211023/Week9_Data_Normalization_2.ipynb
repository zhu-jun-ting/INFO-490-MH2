{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i4do3tmhVfn"
      },
      "source": [
        "#**Data Normalization (part 2)**\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1AroOEeHMu8OFM9bCN7AvrjnPc_UvCvsb)\n",
        "\n",
        "This lesson continues the discussion on data preparation and normalization. What follows are some common techniques we can use to apply rules to help bring some consistency to handling text.\n",
        "\n",
        "One of the underlying issues with the analysis that involved word frequency counters or dictionaries is that a word is separately counted even though it may exist in the dictionary but in a different form. For example, **argue, arguing, argues, argued** would all be distinct keys in our counter even though they are essentially the 'same' word. As we saw in both the lessons on tfâ€¢idf, word embeddings and word2vec, the more features (usually unique words), the more space (longer vectors) will be required to manage each word (i.e. feature).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-qGLmAChlGx"
      },
      "source": [
        "## **Text Cleaning**\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1H44c5xAatjLQrcgdVR502hUqPTZyJ8jb)\n",
        "\n",
        "Cleaning usually means removing or intelligently dealing with errors. But cleaning can also involve processing the text such that down stream users of the data can easily tokenize or process the text.\n",
        "\n",
        "For processing books and other forms of 'printed' media,\n",
        "text, cleaning can involve removing front and back\n",
        "matter, chapter headings, and page numbers. However,\n",
        "if the text is digitized via OCR (optical character recognition), it's quite possible that additional cleaning (even via machine learning) will be needed to deal with any digital artifacts.\n",
        "\n",
        "It's also possible that if the corpus is large or the text documents are long enough, the analysis is no better off by doing additional cleaning. This is not (usually) true when dealing with 'raw' human text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAlisdORiGv5"
      },
      "source": [
        "## **Human Cleaning**\n",
        "\n",
        "When you are dealing with text generated from humans (surveys, emails, transcriptions, web pages, recipes, tweets, txt, chats etc) that doesn't go through a rigorous editing process, additional cleaning can be extremely useful. However, it is extremely difficult. It can include spell correction, making abbreviations consistent (Dr, DR., Doctor, Dr., Doc.), working with text emojis (emoticons) and Emojis (ðŸ˜€), handling poor grammar, improper word usage, sentence structure and inconsistent punctuation (just to name a few).\n",
        "\n",
        "Most systems that process human generated text rely on rule sets as it's usually too time consuming (and costly) and difficult to clean the text manually. Hence, the author's use (or misuse) of slang, puns, idioms, spelling will often lead to a wrong analysis of the authors intended meaning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l27GkvKMiUkP"
      },
      "source": [
        "## **Case normalization, stopwords and cut off lengths**\n",
        "\n",
        "One of easiest processes to reduce the number of unique words in your corpus is to simply make them all lowercase. For most situations 'The' and 'the' are equivalent. Another quick method is to simply drop words that are less than 3 characters long. This cleans up any leftovers from parsing artifacts (having single punctuation 'words').\n",
        "\n",
        "Another effective processing step is to properly handle contractions. Depending on the analysis, the words didn't and 'did not' should be treated equally. Most contractions can be expanded with a few regular expression rules.\n",
        "\n",
        "Removing stop words (words that are so common that they provide no information). Determiners (a, an, another), conjunctions (and, or, but, yet), prepositions (in, of) are all good candidates. Run the following demo that illustrates how one can cut down from 75K words to 5K by using some simple normalization techniques:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4AGMOU8YihB-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "78706\n",
            "75277\n",
            "7019\n",
            "6086\n",
            "6031\n",
            "5898\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import ssl\n",
        "\n",
        "def tokenize(text):\n",
        "    import re\n",
        "\n",
        "    # allow numbers\n",
        "    # reg = r\"['A-Za-z0-9]+-?[A-Za-z0-9']+\"\n",
        "\n",
        "    # exclude numbers\n",
        "    reg = r\"['A-Za-z]+-?[A-Za-z']+\"\n",
        "    regex = re.compile(reg)\n",
        "    return regex.findall(text)\n",
        "\n",
        "def normalize(words):\n",
        "    return [w.lower().strip(\"'\") for w in words]\n",
        "\n",
        "def normalization_demo():\n",
        "    path = \"harryPotter.txt\"\n",
        "    with open(path, 'r') as fd:\n",
        "        all = fd.read()\n",
        "        # the most basic way to tokenize\n",
        "        raw = all.split()\n",
        "        \n",
        "        # use a regular expression to tokenize\n",
        "        words = tokenize(all)\n",
        "        normalized = normalize(words)\n",
        "        \n",
        "        uniq = set(words)\n",
        "        uniq_norm = set(normalized)\n",
        "        uniq_norm_min = set([w for w in uniq_norm if len(w) > 2])\n",
        "        \n",
        "        try:\n",
        "            _create_unverified_https_context = ssl._create_unverified_context\n",
        "        except AttributeError:\n",
        "            pass\n",
        "        else:\n",
        "            ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "        nltk.download('stopwords')\n",
        "        from nltk.corpus import stopwords\n",
        "        stop = stopwords.words('english')\n",
        "        \n",
        "        uniq_no_stop = set([w.lower() for w in uniq_norm_min if w not in stop])\n",
        "        \n",
        "        # some basic counts of the different techniques\n",
        "        print(len(raw))    # 78706\n",
        "        print(len(words))  # 75529 with numbers; 75277 w/out\n",
        "        print(len(uniq))\n",
        "        print(len(uniq_norm))\n",
        "        print(len(uniq_norm_min))\n",
        "        print(len(uniq_no_stop))\n",
        "        # print(sorted(uniq_norm_min))\n",
        "        \n",
        "normalization_demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXOBqGb1ikWo"
      },
      "source": [
        "##**Stemming**\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1JK6jYt-gkmnXKOeQwTSNJUuI_8wvblT4)\n",
        "\n",
        "Stemming is the process of reducing words to a base or root form. However, the result may not be an actual word. The stemming process applies an algorithm in an attempt to get to the root word. One of the easiest transformations to make is to remove suffixes (e.g. 'ed', 'ing', 'ly'). The stemming process can result in some strange words ('ties' becomes 'ti'). Algorithmic stemming has a rich history in computer science and there are multiple algorithms to do so."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtK-Rgj9itXL"
      },
      "source": [
        "**Porter stemmer**\n",
        "\n",
        "The Porter Stemmer algorithm (by Martin Porter) is one of the more popular stemmers. NLTK provides an implementation of it as well. Those interested in the details of the algorithm can [read](http://facweb.cs.depaul.edu/mobasher/classes/csc575/papers/porter-algorithm.html) about them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fGq2TecVjh-n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run        --> run\n",
            "runner     --> runner\n",
            "running    --> run\n",
            "ran        --> ran\n",
            "runs       --> run\n",
            "easily     --> easili\n",
            "fairly     --> fairli\n",
            "children   --> children\n",
            "plotted    --> plot\n",
            "potter     --> potter\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "def porter_test(words):\n",
        "    from nltk.stem.porter import PorterStemmer\n",
        "    p_stemmer = PorterStemmer()\n",
        "    for word in words:\n",
        "        msg = \"{:10s} --> {:s}\".format(word, p_stemmer.stem(word))\n",
        "        print(msg)\n",
        "        \n",
        "words = 'run runner running ran runs easily fairly children plotted potter'.split()\n",
        "porter_test(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDYq-Dldi_e2"
      },
      "source": [
        "**Snowball Stemmer**\n",
        "\n",
        "The [snowball](https://snowballstem.org/) stemmer [historical reference](http://snowball.tartarus.org/) fixes a few of the issues with the Porter algorithm. It is written by the same author. You may also come across 'Porter2' references -- which refers to the same improved algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZCqsiZVkjgLt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run        --> run\n",
            "runner     --> runner\n",
            "running    --> run\n",
            "ran        --> ran\n",
            "runs       --> run\n",
            "easily     --> easili\n",
            "fairly     --> fair\n",
            "children   --> children\n",
            "plotted    --> plot\n",
            "potter     --> potter\n"
          ]
        }
      ],
      "source": [
        "def snowball_test(words):\n",
        "    # Porter2\n",
        "    # The Snowball Stemmer requires that you pass a language parameter\n",
        "    stemmer = nltk.stem.snowball.SnowballStemmer(language='english')\n",
        "    for word in words:\n",
        "        msg = \"{:10s} --> {:s}\".format(word, stemmer.stem(word))\n",
        "        print(msg)\n",
        "snowball_test(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGxoIF2Bjmlk"
      },
      "source": [
        "You can also use Snowball to build your own domain-specific stemmer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sek-fmWyjoxY"
      },
      "source": [
        "**Lancaster Stemmer**\n",
        "\n",
        "The Lancaster stemmer is a very aggressive stemming algorithm, sometimes to a fault. With porter and snowball, the stemmed representations are usually fairly intuitive. However, with Lancaster, many shorter words will become totally obfuscated. It is the fastest algorithm of he three and will reduce your working set of words hugely. If you want more distinction, it is not the tool you would want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0KCok3RUj0hJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run        --> run\n",
            "runner     --> run\n",
            "running    --> run\n",
            "ran        --> ran\n",
            "runs       --> run\n",
            "easily     --> easy\n",
            "fairly     --> fair\n",
            "children   --> childr\n",
            "plotted    --> plot\n",
            "potter     --> pot\n"
          ]
        }
      ],
      "source": [
        "def lancaster_test(words):\n",
        "    stemmer = nltk.stem.lancaster.LancasterStemmer()\n",
        "    for word in words:\n",
        "        msg = \"{:10s} --> {:s}\".format(word, stemmer.stem(word))\n",
        "        print(msg)\n",
        "lancaster_test(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3OkcjX4j5NV"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "> ***Data Scientist Log:*** It's important that you not only run the above code examples, but actually read and interpret their results. Test it with your set of words. How do the different algorithms treat those words?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j6xsC1vj-k2"
      },
      "source": [
        "##**Lemmatization**\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1heucu8-yXYyOWi5867GXViIVZFmzMt4e)\n",
        "\n",
        "Unlike stemming, lemmatization attempts to reduce the word and keep it's part of speech. In linguistics, it is the process of grouping together the different inflected forms of a word and treat the set a single item. Lemmatization looks at surrounding text to determine a given wordâ€™s part of speech.\n",
        "\n",
        "A lemma is the form of the word that usually appears in the\n",
        "dictionary and used to represent other forms of that word. Lemmatization is the algorithmic process of determining the lemma of a word based on its\n",
        "intended meaning.\n",
        "\n",
        "**NLTK Lemmatization via Wordnet**\n",
        "\n",
        "The nltk nlp toolkit has a lemmatizer that uses [Wordnet](https://wordnet.princeton.edu/), a product from Princeton, and is a large database (lexical) of English nouns, verbs, adjectives and adverbs. It also provides Synsets which provides a linked network of related words.\n",
        "\n",
        "The following shows a simple demonstration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "T8UrrF_IkepL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run        --> run\n",
            "runner     --> runner\n",
            "running    --> running\n",
            "ran        --> ran\n",
            "runs       --> run\n",
            "easily     --> easily\n",
            "fairly     --> fairly\n",
            "children   --> child\n",
            "plotted    --> plotted\n",
            "potter     --> potter\n",
            "better     --> good\n",
            "better     --> better\n",
            "better     --> good\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /Users/mac/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/mac/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "def demo_nltk_lemma(words):\n",
        "    import nltk\n",
        "    nltk.download('wordnet')\n",
        "    lemmer  = nltk.stem.WordNetLemmatizer()\n",
        "    for word in words:\n",
        "        msg = \"{:10s} --> {:s}\".format(word, lemmer.lemmatize(word))\n",
        "        print(msg)\n",
        "\n",
        "    # ask for a specific usage\n",
        "    msg = \"{:10s} --> {:s}\".format('better', lemmer.lemmatize('better', pos=\"a\"))\n",
        "    print(msg)\n",
        "    \n",
        "demo_nltk_lemma(words)\n",
        "demo_nltk_lemma(['better'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG0TgEjQkhfx"
      },
      "source": [
        "**Spacy Lemmatization**\n",
        "\n",
        "Spacy has opted to only have lemmatization available instead of having stemming features. The following shows that whey you tokenize a passage of text, each item includes the lemma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "e6YuGwRgkqmX"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/zq/gn09x0d53r74hd7z0lp249cr0000gp/T/ipykernel_12123/3225361248.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'==>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mspacy_lemma_demo1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/var/folders/zq/gn09x0d53r74hd7z0lp249cr0000gp/T/ipykernel_12123/3225361248.py\u001b[0m in \u001b[0;36mspacy_lemma_demo1\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# tokens have a lemma_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     )\n",
            "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")"
          ]
        }
      ],
      "source": [
        "def spacy_lemma_demo1():\n",
        "\n",
        "    import spacy\n",
        "    nlp = spacy.load('en')\n",
        "  \n",
        "    # tokens have a lemma_\n",
        "    doc = nlp(\"Apples are better than ducks\")\n",
        "    for token in doc:\n",
        "      print(token.text, '==>', token.lemma_)\n",
        "\n",
        "spacy_lemma_demo1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWeFNyBPkxYD"
      },
      "source": [
        "You can also use the lemmatizer directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "57dPGasRk0Sv"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'spacy.lemmatizer'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/zq/gn09x0d53r74hd7z0lp249cr0000gp/T/ipykernel_12123/865827757.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mspacy_lemma_demo2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/var/folders/zq/gn09x0d53r74hd7z0lp249cr0000gp/T/ipykernel_12123/865827757.py\u001b[0m in \u001b[0;36mspacy_lemma_demo2\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspacy_lemma_demo2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLemmatizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mADJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNOUN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVERB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy.lemmatizer'"
          ]
        }
      ],
      "source": [
        "def spacy_lemma_demo2():\n",
        "    import spacy\n",
        "    from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\n",
        "    nlp = spacy.load('en')\n",
        "    lemmatizer = nlp.vocab.morphology.lemmatizer\n",
        "    l = lemmatizer('ducks', NOUN)\n",
        "    print(l)\n",
        "\n",
        "spacy_lemma_demo2()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy8AHa7Hk5A_"
      },
      "source": [
        "You can also build your own lemmatizer and add rules depending on your situation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lj8_ZZLJjg1D"
      },
      "outputs": [],
      "source": [
        "def spacy_lemma_demo3():\n",
        "    from spacy.lemmatizer import Lemmatizer\n",
        "    from spacy.lookups import Lookups\n",
        "    lookups = Lookups()\n",
        "    # add a custom conversion for all nouns\n",
        "    lookups.add_table(\"lemma_rules\", {\"noun\": [[\"s\", \"\"]]})\n",
        "    lemmatizer = Lemmatizer(lookups)\n",
        "    lemmas = lemmatizer(\"ducks\", \"NOUN\")\n",
        "    print(lemmas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdqd7ndVlA4J"
      },
      "source": [
        "Additionally Gensim, TextBlob, and Stanford's CoreNLP provide lemmatizers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8xCT-KlLcP"
      },
      "source": [
        "#**Lesson Assignment**\n",
        "Although there is no assignment, make sure you understand and learn the concepts taught in this lesson.\n",
        "\n",
        "<h1><center>The End!</center></h1>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Week9_Data Normalization_2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
